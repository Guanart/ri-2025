{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4435cb51",
   "metadata": {},
   "source": [
    "# Ley de Zipf: Poda del vocabulario\n",
    "En este notebook se calcula la cantidad de palabras que deberían estar en el 10%, 20% y 30% del vocabulario según la Ley de Zipf. Luego, se poda el vocabulario en esos porcentajes y se analiza:\n",
    "- Qué porcentaje de las palabras podadas coincide con *stopwords*.\n",
    "- Qué palabras podadas no son *stopwords* y si podrían ser importantes para la recuperación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3622fd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de términos únicos: 26645\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Tokenizador import Tokenizador\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Tokenizar y calcular frecuencias\n",
    "tokenizador = Tokenizador(stopwords_path=\"stopwords.txt\")\n",
    "resultados = tokenizador.analizar_coleccion(\"datos/\")\n",
    "\n",
    "output_dir = \"resultados/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "tokenizador.generar_archivos_salida(resultados, output_dir)\n",
    "\n",
    "# Ordenar términos por frecuencia descendente\n",
    "sorted_terms = sorted(\n",
    "    [(termino, data[\"cf\"]) for termino, data in resultados[\"terminos\"].items()],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Total de términos únicos\n",
    "total_terminos = len(sorted_terms)\n",
    "print(f\"Total de términos únicos: {total_terminos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793377d",
   "metadata": {},
   "source": [
    "## Calcular palabras esperadas en el 10%, 20% y 30% del vocabulario según la Ley de Zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a8133c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes de la recta log-log: [-1.15691    11.44913022]\n"
     ]
    }
   ],
   "source": [
    "ranks = list(range(1, len(sorted_terms) + 1))  # 1, 2, 3, ...\n",
    "freqs = [freq for _, freq in sorted_terms]\n",
    "log_ranks = np.log(ranks)\n",
    "log_freqs = np.log(freqs)\n",
    "coef = np.polyfit(x=log_ranks, y=log_freqs, deg=1)  # parámetros de la ecuación de zipf\n",
    "print(\"Coeficientes de la recta log-log:\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f23c348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros de la ley de Zipf: C = 93819.70821221864, b = -1.1569099990102645\n",
      "Cantidad de tokens en el 10% del vocabulario (según Ley de Zipf): 479703.0094362753\n",
      "Cantidad de tokens en el 10% del vocabulario (real): 332707\n",
      "Cantidad de tokens en el 20% del vocabulario (según Ley de Zipf): 497578.5236158122\n",
      "Cantidad de tokens en el 20% del vocabulario (real): 351787\n",
      "Cantidad de tokens en el 30% del vocabulario (según Ley de Zipf): 507164.886312633\n",
      "Cantidad de tokens en el 30% del vocabulario (real): 361412\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Respuesta profe: si el vocabulario tiene una longitud de 1000 palabras y quisieras estimar cuántas palabras hay en las primeras 100 posiciones del ranking (10% del vocabulario), se puede sumar las frecuencias de cada palabra desde r = 1 hasta r=100, donde la frecuencia para cada posición viene dada por:\n",
    "\n",
    "frec(r) = C.r^b (reemplazando C y b por los obtenidos en el punto 7)\n",
    "'''\n",
    "\n",
    "# Estimar la cantidad de palabras en el 10% del vocabulario utilizando la ley de Zipf\n",
    "def estimar_frecuencia_zipf(r, C, b):\n",
    "    return C * (r**b)\n",
    "# Parámetros de la ley de Zipf\n",
    "C = np.exp(coef[1])\n",
    "b = coef[0]\n",
    "\n",
    "print(f\"Parámetros de la ley de Zipf: C = {C}, b = {b}\")\n",
    "\n",
    "for percent in [0.1, 0.2, 0.3]:\n",
    "    n = int(total_terminos * percent)   # Número de términos a estimar (10% del vocabulario)\n",
    "    # Frecuencias estimadas\n",
    "    frecuencias_estimadas = [estimar_frecuencia_zipf(r, C, b) for r in range(1, n + 1)]\n",
    "    print(f\"Cantidad de tokens en el {percent*100:.0f}% del vocabulario (según Ley de Zipf): {sum(frecuencias_estimadas)}\")\n",
    "    print(f\"Cantidad de tokens en el {percent*100:.0f}% del vocabulario (real): {sum(freqs[:n])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31673bc2",
   "metadata": {},
   "source": [
    "## Poda del vocabulario en 10%, 20% y 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f293d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Poda del vocabulario al 10%:\n",
      "Palabras podadas: 2664\n",
      "Palabras podadas que no son stopwords: 2230\n",
      "Porcentaje de palabras podadas que no son stopwords: 83.71%\n",
      "\n",
      "Poda del vocabulario al 20%:\n",
      "Palabras podadas: 5329\n",
      "Palabras podadas que no son stopwords: 4809\n",
      "Porcentaje de palabras podadas que no son stopwords: 90.24%\n",
      "\n",
      "Poda del vocabulario al 30%:\n",
      "Palabras podadas: 7993\n",
      "Palabras podadas que no son stopwords: 7428\n",
      "Porcentaje de palabras podadas que no son stopwords: 92.93%\n"
     ]
    }
   ],
   "source": [
    "def podar_vocabulario(terminos, porcentaje):\n",
    "    n = int(len(terminos) * porcentaje)\n",
    "    return dict(terminos[:n])\n",
    "\n",
    "# Poda del vocabulario al 10%, 20% y 30%\n",
    "for percent in [0.1, 0.2, 0.3]:\n",
    "    terminos_podados = podar_vocabulario(sorted_terms, percent)\n",
    "\n",
    "    # Verificar qué porcentaje de la poda coincide con palabras vacías. Extraiga las palabras podadas que no son *stopwords* y verifique si, a su criterio, pueden ser importantes para la recuperación.\n",
    "    stopwords = set(tokenizador.stopwords)\n",
    "    terminos_no_stopwords = {termino: data for termino, data in terminos_podados.items() if termino not in stopwords}\n",
    "    print(f\"\\nPoda del vocabulario al {percent*100:.0f}%:\")\n",
    "    print(f\"Palabras podadas: {len(terminos_podados)}\")\n",
    "    print(f\"Palabras podadas que no son stopwords: {len(terminos_no_stopwords)}\")\n",
    "    print(f\"Porcentaje de palabras podadas que no son stopwords: {len(terminos_no_stopwords) / len(terminos_podados) * 100:.2f}%\")\n",
    "    # for termino in terminos_no_stopwords:\n",
    "    #     print(termino)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
